{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99ad4ea-e02c-49d6-a7c3-37b2cd2bfcd8",
   "metadata": {},
   "source": [
    "# Text_Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87b73e-70b0-43e7-93b7-8c29e1a01eb4",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9382fad-a897-432f-a516-1251e93408ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d3c6201-30f9-4229-89ba-5ac9037454ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is a cheery as cheery goes for a pastry shop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8732ae3-eb2a-484e-8324-970aa00c27ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Jone',\n",
       " \"'s\",\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d927eda-21b2-480a-82f4-986f6587070c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Jone',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19cf8071-7d5b-4538-bcdc-23c3dc5f1b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "070b8f50-34e4-4841-b259-890a951958df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "694803c7-4608-4395-b5ff-1f125f8d0562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cb49d51-7e3b-48a1-b08d-394195666541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I',\n",
       "  'am',\n",
       "  'activly',\n",
       "  'looking',\n",
       "  'for',\n",
       "  'Ph.D.',\n",
       "  'students',\n",
       "  '.',\n",
       "  'and',\n",
       "  'you',\n",
       "  'are',\n",
       "  'a',\n",
       "  'Ph.D.',\n",
       "  'student'],\n",
       " [('I', 'PRP'),\n",
       "  ('am', 'VBP'),\n",
       "  ('activly', 'JJ'),\n",
       "  ('looking', 'VBG'),\n",
       "  ('for', 'IN'),\n",
       "  ('Ph.D.', 'NNP'),\n",
       "  ('students', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('and', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('Ph.D.', 'NNP'),\n",
       "  ('student', 'NN')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am activly looking for Ph.D. students. and you are a Ph.D. student\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "tokenized_sentence, pos_tag(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c6ae9f-f2da-455e-912b-7ab25ffd61ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석 ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅 [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출 ['코딩', '당신', '연휴', '여행']\n",
      "--------------------------------------------------\n",
      "꼬꼬마 형태소 분석 ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "꼬꼬마 품사 태깅 [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "꼬꼬마 명사 추출 ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "print(f\"OKT 형태소 분석 {okt.morphs(text)}\")\n",
    "print(f\"OKT 품사 태깅 {okt.pos(text)}\")\n",
    "print(f\"OKT 명사 추출 {okt.nouns(text)}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"꼬꼬마 형태소 분석 {kkma.morphs(text)}\")\n",
    "print(f\"꼬꼬마 품사 태깅 {kkma.pos(text)}\")\n",
    "print(f\"꼬꼬마 명사 추출 {kkma.nouns(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f05c4-2fe7-46aa-b13d-4a44f3579cc2",
   "metadata": {},
   "source": [
    "## Cleaning & Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d124c43-61a7-4944-b737-dbcaa0fa12da",
   "metadata": {},
   "source": [
    "### Lemmatization (표제어 추출) - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94423f5-5df0-4536-b0a2-1a2e7e4d247b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before abstracting lemmatizer : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "After abstracting lemmatizer : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "## WordNetLemmatizer가 단어의 품사를 모를 때\n",
    "\n",
    "print(f\"Before abstracting lemmatizer : {words}\")\n",
    "print(f\"After abstracting lemmatizer : {[lemmatizer.lemmatize(word) for word in words]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d869510-6ffb-4853-ae55-9e605a2659e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die\n",
      "watch\n",
      "have\n"
     ]
    }
   ],
   "source": [
    "## WordNetLemmatizer가 단어의 품사를 알 때\n",
    "\n",
    "print(lemmatizer.lemmatize('dies', 'v'))\n",
    "print(lemmatizer.lemmatize('watched', 'v'))\n",
    "print(lemmatizer.lemmatize('has', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aea0d6-e6f3-4b6b-8ba2-227cf4fd541e",
   "metadata": {},
   "source": [
    "### Stemming (어간 추출) - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e7dedb-4b9c-43d2-9ce1-9f6f971f47c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before abstracting Stem : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "After abstracting Stem : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print(f\"Before abstracting Stem : {tokenized_sentence}\")\n",
    "print(f\"After abstracting Stem : {[stemmer.stem(word) for word in tokenized_sentence]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459e20c6-74b7-4f21-93f1-ffdf581b0b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before abstracting Stem : ['formalize', 'allowance', 'electricical']\n",
      "After abstracting Stem : ['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical']\n",
    "\n",
    "print(f'Before abstracting Stem : {words}')\n",
    "print(f'After abstracting Stem : {[stemmer.stem(word) for word in words]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc44199-e27e-4cd8-a313-ed6a97d63ba7",
   "metadata": {},
   "source": [
    "### Removing Stopword (불용어) - Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a9658f-a0ac-4da4-98cf-4da6cc7925b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f3fc5f0-6abd-4cce-b8f8-d015e0b93232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Stopword : 179\n",
      "10 Stopwords : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "stop_word_list = stopwords.words('english')\n",
    "print(f\"Count of Stopword : {len(stop_word_list)}\")\n",
    "print(f\"10 Stopwords : {stop_word_list[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a79cadf-68b4-4b96-b634-60a4d6a5a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Removing Stopword : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "After Removing Stopword : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        result.append(word)\n",
    "\n",
    "print(f\"Before Removing Stopword : {word_tokens}\")\n",
    "print(f\"After Removing Stopword : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253206a-92ad-48b9-b5b6-c1491bfcfb02",
   "metadata": {},
   "source": [
    "### Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3647ba7f-6cd9-45b1-8534-ef722fc45604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a02e57e2-0dd4-450d-bd99-9519b4fd8d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A barber is a person.',\n",
       " 'a barber is good person.',\n",
       " 'a barber is huge person.',\n",
       " 'he Knew A Secret!',\n",
       " 'The Secret He Kept is huge secret.',\n",
       " 'Huge secret.',\n",
       " 'His barber kept his word.',\n",
       " 'a barber kept his word.',\n",
       " 'His barber kept his secret.',\n",
       " 'But keeping and keeping such a huge secret to himself was driving the barber crazy.',\n",
       " 'the barber went up a huge mountain.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(raw_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0894089-be0e-4127-bb8c-38dcdb288f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1\n",
    "\n",
    "    preprocessed_sentences.append(result)\n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dc90156-9ad7-4879-897a-9ecd0b17fc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Zip : {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word Zip : {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d03a4e4f-8867-4d4a-9cc5-69ea4b2c6a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81ba5a-41f4-487a-abf7-d163d30cb857",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b36e137-a883-4c85-9d9e-b2a2fdb29b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchtext.data import get_tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bbc9f9f-57db-4505-bfce-cff6e739417a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'barber',\n",
       " 'is',\n",
       " 'a',\n",
       " 'person',\n",
       " '.',\n",
       " 'a',\n",
       " 'barber',\n",
       " 'is',\n",
       " 'good',\n",
       " 'person',\n",
       " '.',\n",
       " 'a',\n",
       " 'barber',\n",
       " 'is',\n",
       " 'huge',\n",
       " 'person',\n",
       " '.',\n",
       " 'he',\n",
       " 'knew',\n",
       " 'a',\n",
       " 'secret',\n",
       " '!',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'he',\n",
       " 'kept',\n",
       " 'is',\n",
       " 'huge',\n",
       " 'secret',\n",
       " '.',\n",
       " 'huge',\n",
       " 'secret',\n",
       " '.',\n",
       " 'his',\n",
       " 'barber',\n",
       " 'kept',\n",
       " 'his',\n",
       " 'word',\n",
       " '.',\n",
       " 'a',\n",
       " 'barber',\n",
       " 'kept',\n",
       " 'his',\n",
       " 'word',\n",
       " '.',\n",
       " 'his',\n",
       " 'barber',\n",
       " 'kept',\n",
       " 'his',\n",
       " 'secret',\n",
       " '.',\n",
       " 'but',\n",
       " 'keeping',\n",
       " 'and',\n",
       " 'keeping',\n",
       " 'such',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'secret',\n",
       " 'to',\n",
       " 'himself',\n",
       " 'was',\n",
       " 'driving',\n",
       " 'the',\n",
       " 'barber',\n",
       " 'crazy',\n",
       " '.',\n",
       " 'the',\n",
       " 'barber',\n",
       " 'went',\n",
       " 'up',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'mountain',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenizer(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a7a44c-9919-4775-b920-0eea2ec67941",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70f5a6ab-7439-4a36-852d-576cdfc44372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "tokens = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e52f9f3f-a810-404a-89ff-f1ad2e1d3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word : index for index, word in enumerate(tokens)}\n",
    "print(f\"단어 집합 : {word_to_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83c0b376-625e-4c24-a1ad-c6c2fcf7c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word_to_index):\n",
    "    one_hot_vector = [0] * (len(word_to_index))\n",
    "    index = word_to_index[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0951f0c7-3acb-4768-b8d4-dd91987f7bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\", word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b5346-36f8-4dd5-9583-f2c4a5fb07eb",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding의 한계 <br> 1. 단어의 개수 증가할수록 요구되는 벡터 차원 수 증가로 인한 메모리 비효율 <br> 2. 단어의 유사도 표현 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66a741-4531-4d09-ab8d-12ee0c16a2c4",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding의 한계 해결 -> LSA, HAL (Count Based) | NNLM, RNNLM, Word2Vec, FastText (Prediction Based)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
